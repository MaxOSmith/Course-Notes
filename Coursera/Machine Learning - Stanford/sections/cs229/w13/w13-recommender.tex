\subsubsection{Problem Formulation}
\begin{itemize}[--]
	\item Example: predicting movie ratings. Want to recommend movies that people will enjoy.
	\item User rates movies using zero-to-five stars \\
	\begin{center}\begin{tabular}{c | c c c c}
		\textbf{Movie} & \textbf{Alice (1)} & \textbf{Bob (2)} & \textbf{Carol (3)} & \textbf{Dave (4)} \\ \hline
		Love at last & 5 & 5 & 0 & 0 \\
		Romance forever & 5 & ? & ? & 0 \\
		Cute puppies of love & ? & 4 & 0 & ? \\
		Nonstop car chases & 0 & 0 & 5 & 4 \\
		Swords vs. karate & 0 & 0 &  5 & ?
	\end{tabular}\end{center}
	\item Notation
	\begin{itemize}[--]
		\item $n_u=$ no. users
		\item $n_m=$ no. movies
		\item $r(i,j)=$ 1 if user $j$ has rated movie $i$
		\item $y^{(i,j)}=$ rating given by user $j$ to movie $i$ (defined only if $r(i,j)=1$)
	\end{itemize}
	\item We want to predict what the `?' will be in our table, namely, how much they will like the movie
	\item In our data, we can see Alice and Bob like romance movies, and not so much action movies. We want to be able to learn from this and recommend movies intelligently based on this sort of information
\end{itemize}

\subsubsection{Content Based Recommendations}
\begin{itemize}[--]
	\item Say we define features based on different recommendation categories: \\
	\begin{center}\begin{tabular}{c | c c c c | c c}
		\textbf{Movie} & \textbf{Alice (1)} & \textbf{Bob (2)} & \textbf{Carol (3)} & \textbf{Dave (4)} & $x_1$ (romance) & $x_2$ (action\\ \hline
		Love at last & 5 & 5 & 0 & 0 & 0.9 & 0\\
		Romance forever & 5 & ? & ? & 0 & 1 & 0.01\\
		Cute puppies of love & ? & 4 & 0 & ? & 0.99 & 0\\
		Nonstop car chases & 0 & 0 & 5 & 4 & 0.1 & 1\\
		Swords vs. karate & 0 & 0 &  5 & ? & 0 & 0.9
	\end{tabular}\end{center}
	\item With this dat we have: $n_u=4, n_m=5$. We also include our intercept term: $x_0=1$
		$$x^{(1)}=\begin{bmatrix}
			1\\
			0.9\\
			0
		\end{bmatrix}$$
	\item For each user $j$, learn a paramter $\theta^{(j)}\in\mathbb{R}^3$. Predict user $j$ as rating movei $i$ with $(\theta^{(j)})^T x^{(i)}$ stars.
	\item Suppose we want to want to guess what Alice will think of ``Cute puppies of love'':
		$$x^{(3)}=\begin{bmatrix}
			1 \\ 0.99 \\ 0
		\end{bmatrix}, \theta^{(1)}=\begin{bmatrix}
			0 \\ 5 \\ 0
		\end{bmatrix}, (\theta^{(1)})^T x^{(3)} = 4.95$$
	\item We will discuss how we got $\theta$ later.
	\item We would predict that Alice would enjoy the movie 
	\item Problem formulation:
	\begin{itemize}[--]
		\item $r(i, j)=1$ if user $j$ has rated movie $i$ (0 otherwise)
		\item $y^{(i,j)}=$ rating by user $j$ on movie $i$ (if defined)
		\item $\theta^{(j)}=$ parameter vector for user $j$
		\item $x^{(i)}=$ feature vector for movei $i$
		\item For user $j$, movie $i$, predicted rating: $(\theta^{(j)})^T x^{(i)}$
		\item $m^{(j)}=$ no. of movies rated by user $j$
		\item To learn $\theta^{(j)}$ (paramter for user $j$):
			$$\min_{\theta^{(j)}}\frac{1}{2m^{(j)}}\sum_{i:r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2m^{(j)}}\sum_{k=1}^n (\theta_k^{(j)})^2$$
		\item To learn $\theta^{(1)},\ldots, \theta^{(n_u)}$:
			$$\min_{\theta^{(1)},\ldots, \theta^{(n_u)}}\frac{1}{2m^{(j)}}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2m^{(j)}}\sum_{j=1}^{n_u}\sum_{k=1}^n (\theta_k^{(j)})^2$$
		\item Gradient descent update:
			$$\theta_k^{(j)}:=\theta_k^{(j)} - \alpha\sum_{i:r(i,j)=1}((\theta^{(j)})^T x^{(i)} - y^{(i,j)})x_k^{(i)} \text{ for } k =0$$
			$$\theta_k^{(j)}:=\theta_k^{(j)} - \alpha ( \sum_{i:r(i,j)=1}((\theta^{(j)})^T x^{(i)} - y^{(i,j)})x_k^{(i)} +\lambda\theta_k^{(j)} ) \text{ for } k \neq 0$$
			
	\end{itemize}
	\item Since $m^{(j)}$ is a constant, you may exclude it to simplify the math
\end{itemize}

\subsubsection{Collaborative Filtering}
\begin{itemize}[--]
	\item One of the movies has a feature vector $x^{(1)}$, we can infer from the opinions (the $\theta$) of the individuals the type of movie we are looking at
	\item Given $\theta^{(1)},\ldots, \theta^{(n_u)}$, to learn $x^{(i)}$:
		$$\min_{x^{(i)}}\frac{1}{2}\sum_{j:r(i,j)=1}((\theta^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2}\sum_{k=1}^n (x_k^{(i)})^2$$
	\item Given $\theta^{(1)},\ldots,\theta^{(n_u)}$ to learn $x^{(1)},\ldots , x^{(n_m)}$:
		$$\min_{x^{(i)}}\frac{1}{2}\sum_{i=1}^{n_m}\sum_{j:r(i,j)=1}((\theta^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^n (x_k^{(i)})^2$$
	\item Given $x^{(n_m)}$ (and omvie ratings), can estimate $\theta^{(n_u)}$
	\item Given $\theta^{(n_u)}$, can estimate $x^{(n_m)}$
	\item Which do we want to do first?
	\item Guess $\theta\to x\to\theta\to x\to\ldots$
	\item This will cause your features to converge via \textbf{collaborative filtering}
\end{itemize}

\subsubsection{Collaborative Filtering Algorithm}
\begin{itemize}[--]
	\item Collaborative filtering optimization objective:
		$$J(x^{(1)},\ldots,x^{(n_m)},\theta^{(1)},\ldots,\theta^{(n_u)}=\frac{1}{2}\sum_{(i,j):r(i,j)=1}((\theta^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n} (x_k^{(i)})^2 + \frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n} (\theta_k^{(j)})^2$$

		$$\min_{x^{(1)},\ldots, x^{(n_m)}, \theta^{(1)},\ldots,x^{(n_u)}} J(\ldots)$$

	\item Algorithm:
	\begin{itemize}[--]
		\item Initialize $x^{(n_m)}, \theta^{(n_u)}$ to small random values
		\item Minimize $J(\ldots)$ using gradient descent (or an advanced optimization algorithm) eg. for every $j=1,\ldots,n_u, i=1,\ldots n_m$:
			$$x_k^{(i)}:= x_k^{(i)} - \alpha \left(
				\sum_{j:r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)})\theta_k^{(j)} +\lambda x_k^{(i)}
			\right)$$
			$$\theta_k^{(j)}:=\theta_k^{(j)} - \alpha \left(
				\sum_{i:r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)})x_k^{(i)} +\lambda \theta_k^{(j)}
			\right)$$
		\item For a user with parameters $\theta$ and a movie with (learned) features $x$, predict a star rating of $\theta^T x$
	\end{itemize}
\end{itemize}

\subsubsection{Vectorization - Low Rank Matrix Factorization}
\begin{itemize}[--]
	\item Lets group the movie data into a matrix:
		$$Y=\begin{bmatrix}
			5 & 5 & 0 & 0 \\
			5 & ? & ? & 0 \\
			? & 4 & 0 & ? \\
			0 & 0 & 5 & 4 \\
			0 & 0 & 5 & 0
		\end{bmatrix}$$
	\item This is what we were previously writing sa $y^{(i,j)}$
	\item Predicted ratings:
		$$\begin{bmatrix}
			(\theta^{(1)})^Tx^{(1)} & (\theta^{(2)})^Tx^{(1)} & \ldots & (\theta^{(n_u)})^Tx^{(1)} \\
			(\theta^{(1)})^Tx^{(2)} & (\theta^{(2)})^Tx^{(2)} & \ldots & (\theta^{(n_u)})^Tx^{(2)} \\
			\vdots & \vdots & \ddots & \vdots \\
			(\theta^{(1)})^Tx^{(n_m)} & (\theta^{(2)})^Tx^{(n_m)} & \ldots & (\theta^{(n_u)})^Tx^{(n_m)}
		\end{bmatrix}$$\
	\item We can denote vectorize this via:
		$$X=\begin{bmatrix}
			\rule[.5ex]{.5em}{0.4pt} & (x^{(1)})^T & \rule[.5ex]{.5em}{0.4pt} \\
			\rule[.5ex]{.5em}{0.4pt} & (x^{(2)})^T & \rule[.5ex]{.5em}{0.4pt} \\
			& \vdots \\
			\rule[.5ex]{.5em}{0.4pt} & (x^{(n_m)})^T & \rule[.5ex]{.5em}{0.4pt} \\
		\end{bmatrix}, \Theta=\begin{bmatrix}
			\rule[.5ex]{.5em}{0.4pt} & (\theta^{(1)})^T & \rule[.5ex]{.5em}{0.4pt} \\
			\rule[.5ex]{.5em}{0.4pt} & (\theta^{(2)})^T & \rule[.5ex]{.5em}{0.4pt} \\
			& \vdots \\
			\rule[.5ex]{.5em}{0.4pt} & (\theta^{(n_u)})^T & \rule[.5ex]{.5em}{0.4pt} \\
		\end{bmatrix}$$
		$$X\Theta^T$$
	\item This algorithm is also called \textbf{low rank matrix factorization}
	\item This matrix has a mathematical property called \textbf{low rank matrix}
	\item Finding related movies:
	\begin{itemize}[--]
		\item For each product $i$, we learn a feature vector $x^{(i)}\in\mathbb{R}^n$
		\item Potential features: $x_1 = $ romance, $x_2 = $ action, $\ldots$
		\item It's hard to truely interpret for a human in practice
		\item How to find movies $j$ related to movie $i$?
		\item $||x^{(i)} - x^{(j)}||$ is small $\to$ movie $j$ and $i$ are ``similar'' 
	\end{itemize}
\end{itemize}

\subsubsection{Implementaiton Detail - Mean Normalization}
\begin{itemize}[--]
	\item Consider a user who has not rated any movies
	\item $n=2, \theta^{(5)}\in\mathbb{R}^2$, but he has no movies which are rated
	\item So only the regularization term afffects $\theta^{(5)}$, since the first term in the cost function is unknown
	\item This will result in: $\theta^{(5)}=\vec{0}$
	\item This assumption of ranking all movies with a 0, doesn't seem very likely
	\item For \textbf{mean normalization} we will compute the average rating as so:
		$$\mu=\begin{bmatrix}
			2.5 \\ 2.5 \\ 2 \\ 2.25 \\ 1.25
		\end{bmatrix}$$
	\item Then we perform: $Y:= Y - \mu$
	\item We will use these mean normalized movie ratings to learn $\theta^{(j)}, x^{(i)}$
	\item For user $j$, on movie $i$ predict:
		$$(\theta^{(j)})^Tx^{(i)} + \mu_i$$
	\item This will result in our new user to have average ratings as a guess
 \end{itemize}